[dataset]
sampling_rate = 22050
n_fft = 1024
win_length = 1024
hop_length = 256
n_mels = 80
f_min = 0
f_max = 8000
dio_speed = 4
# 1 = no smoothing
pitch_smooth = 1
# can be "kaldi" or "pyworld"
pitch_type = pyworld
remove_outliers = False
target_lang = None
remove_stress = True
source_phoneset = arpabet
variance_level = phoneme
cache_dir = ../.cache/lfs2dataset

[model]
encoder_head = 2
encoder_layers = 4
encoder_hidden = 256
encoder_dropout = 0.1
encoder_kernels = [9, 9, 9, 9]
conv_filter_size = 1024
variance_nbins = 256
variance_filter_size = 256
variance_kernel = 3
variance_dropout = 0.5
tgt_max_length = 4096
decoder_head = 2
decoder_layers = 4
decoder_hidden = 256
decoder_dropout = 0.1
decoder_kernels = [9, 9, 9, 9]
mel_channels = 80
postnet = False
dvector = True
snr = True
conditioned = True
depthwise_conv = True
# duration transformer
duration_transformer = False
duration_transformer_hidden = 128
duration_transformer_head = 1
# stochastic duration predictor
duration_stochastic = True
duration_stochastic_hidden = 192
duration_stochastic_kernel = 3
duration_stochastic_dropout = 0.25
duration_stochastic_num_flows = 4
duration_stochastic_conditioning_size = 256

[train]
lr = 5e-03
epochs = 100
batch_size = 6
gradient_accumulation = 5
gradient_clipping = 1
train_path = ../Data/LibriTTS/train-clean-360-aligned
stats_path = ../Data/LibriTTS/train-clean-360-aligned/stats.json
#../Data/Synthesised/n_topline_fixed
#train_path = ../Data/LibriTTS/train-clean-100-aligned+../Data/LibriTTS/train-clean-360-aligned
valid_path = ../Data/LibriTTS/dev-clean-aligned
validation_step = 1.0
wandb_mode = online
distributed = True
#model_path = None
model_path = models/flow14.ckpt
augment_duration = 0.1
# TODO: use this config value
# models/dvector.ckpt

[inference]
duration_stochastic_sigma = 1.0